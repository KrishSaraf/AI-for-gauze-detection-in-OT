{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3f4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\victo/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-3-1 Python-3.11.4 torch-2.2.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2060 with Max-Q Design, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to capture frames from cameras\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "combined_model = torch.hub.load('ultralytics/yolov5', 'custom', path=r'C:\\\\Users\\\\victo\\\\Desktop\\\\2 Categories Yolov5.pt',force_reload=True)\n",
    "combined_model.cuda()\n",
    "combined_model.amp=False\n",
    "combined_model.conf = 0.20  # Confidence threshold\n",
    "combined_model.classes = [0, 1]  # Class 0 for gauze, 1 for hand\n",
    "\n",
    "# Initialize variables for both cameras\n",
    "onscreenIn, onscreenOut, countIn, countOut, startTime, endTime, countPlay = 0, 0, 0, 0, time.time(), 0, 0\n",
    "frameCountIn, frameCountOut = 0, 0\n",
    "isPaused = False\n",
    "condition = 7\n",
    "\n",
    "\n",
    "# Initialize video capture for two cameras\n",
    "#cap1 = cv2.VideoCapture(0)  # First camera\n",
    "cap1 = cv2.VideoCapture(\"C:\\\\Users\\\\victo\\\\Pictures\\\\Camera Roll\\\\WIN_20240214_19_38_37_Pro.mp4\")  # First Video\n",
    "cap2 = cv2.VideoCapture(1)  # Second camera\n",
    "\n",
    "# Display resolution and aspect ratio calculations\n",
    "display_width_per_camera = 1920 // 2\n",
    "display_height_per_camera = int(display_width_per_camera * (9 / 16))\n",
    "display_height = display_height_per_camera\n",
    "\n",
    "# Initialize the dictionary to track the previous key states\n",
    "prev_keys = {}\n",
    "\n",
    "PAUSE_DURATION = 2  # in seconds\n",
    "UNPAUSE_DURATION = 0\n",
    "pause_timer = 0  # Timer variable to track the pause duration\n",
    "unpause_timer = 0 \n",
    "\n",
    "while True:\n",
    "    # Read frames from both cameras\n",
    "    ret, frame1 = cap1.read()\n",
    "    ret1, frame2 = cap2.read()\n",
    "\n",
    "    if not ret or not ret1:\n",
    "        print(\"Failed to capture frames from cameras\")\n",
    "        break\n",
    "\n",
    "    frame1 = cv2.resize(frame1, (display_width_per_camera, display_height_per_camera))\n",
    "    frame2 = cv2.resize(frame2, (display_width_per_camera, display_height_per_camera))\n",
    "\n",
    "    combined_frame = np.hstack((frame1, frame2))\n",
    "\n",
    "    # Make detections\n",
    "    results = combined_model(combined_frame)\n",
    "\n",
    "    prevOnScreenIn, prevOnScreenOut = onscreenIn, onscreenOut\n",
    "    a, b, handDetected = 0, 0, False\n",
    "    # Get the current key press\n",
    "    key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "    # Function to handle key press\n",
    "    def handle_key_press(key_char, action):\n",
    "        if prev_keys.get(key_char, -1) != key and key == ord(key_char):\n",
    "            action()\n",
    "            prev_keys[key_char] = key\n",
    "        elif key != ord(key_char):\n",
    "            prev_keys[key_char] = -1\n",
    "\n",
    "    # Correcting the lambda functions to properly modify a and b\n",
    "    handle_key_press('0', lambda: globals().update(countIn=countIn+1))   #a=a+1))\n",
    "    handle_key_press('1', lambda: globals().update(countIn=countIn-1))   #a=a-1 ))#if a > 0 else 0))\n",
    "    handle_key_press('2', lambda: globals().update(countOut=countOut+1))   #b=b+1))\n",
    "    handle_key_press('3', lambda: globals().update(countOut=countOut-1))   #b=b-1 ))#if b > 0 else 0))\n",
    "\n",
    "    for detection in results.xyxy[0]:\n",
    "        detected_class = detection[5].item()\n",
    "        if detected_class == 0:\n",
    "            if detection[0].item() < frame1.shape[1]:\n",
    "                a += 1\n",
    "            else:\n",
    "                b += 1\n",
    "        elif detected_class == 1:\n",
    "            handDetected = True\n",
    "    \n",
    "    #isPaused = handDetected\n",
    "    \n",
    "    if time.time() - unpause_timer >= UNPAUSE_DURATION:\n",
    "        isPaused = True\n",
    "    \n",
    "    if handDetected:\n",
    "        pause_timer = time.time()\n",
    "        isPaused = True\n",
    "    elif pause_timer > 0 and time.time() - pause_timer >= PAUSE_DURATION:\n",
    "        isPaused = False\n",
    "        pause_timer = 0\n",
    "        unpause_timer = time.time()\n",
    "    \n",
    "    \n",
    "        \n",
    "    if not isPaused:\n",
    "        onscreenIn, onscreenOut = a, b\n",
    "        if onscreenIn > prevOnScreenIn:\n",
    "            countIn += onscreenIn - prevOnScreenIn\n",
    "        if onscreenOut > prevOnScreenOut:\n",
    "            countOut += onscreenOut - prevOnScreenOut\n",
    "        countPlay = countIn - countOut - onscreenIn\n",
    "    \n",
    "    endTime = time.time()\n",
    "    fps = 1 / (endTime - startTime)\n",
    "    startTime = endTime\n",
    "\n",
    "    image = np.squeeze(results.render())\n",
    "\n",
    "    # Apply text overlays\n",
    "    cv2.putText(image, f'On Screen In = {onscreenIn}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f'Total In = {countIn}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f'On Screen Out = {onscreenOut}', (display_width_per_camera + 10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f'Total Out = {countOut}', (display_width_per_camera + 10, 70),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f'In Play = {countPlay}', (960 - 100, display_height - 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.putText(image, f'FPS = {round(fps, 1)}', (960 - 100, display_height - 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    if isPaused:\n",
    "        cv2.putText(image, 'System Paused - Hand Detected', (960 - 300, display_height // 2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow('Gauze Detection', image)\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef77cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
